{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIL-Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berniwal/CIL_Project/blob/master/CIL-Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SIsoqF2sKj",
        "colab_type": "text"
      },
      "source": [
        "## Go to correct directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IzoDHLY-ykE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "07d93c59-2091-4a50-f92f-315e87eabf68"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/CIL')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/CIL'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXAkBoig2yy-",
        "colab_type": "text"
      },
      "source": [
        "## install + import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep6izaAW_JwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61bd6b71-f8bb-4691-b1ff-d86daf80af3b"
      },
      "source": [
        "!pip install params_flow==0.7.1\n",
        "!pip install py-params==0.7.3\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "import bert\n",
        "#different Tokenizer for Albert\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer\n",
        "from bert.tokenization.albert_tokenization import AlbertFullTokenizer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert import BertModelLayer\n",
        "import sentencepiece as spm\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting params_flow==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/81/ae/c286650d720d3b3f7e6c758d5af9ab62dc5b4c0d1c3d910baccb1be6f70f/params-flow-0.7.1.tar.gz\n",
            "Collecting py-params>=0.6.4\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params_flow==0.7.1) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params_flow==0.7.1) (4.41.1)\n",
            "Building wheels for collected packages: params-flow, py-params\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.7.1-cp36-none-any.whl size=15376 sha256=a0a7d6420b923d3281d5c2f8e66661abc389887163a03e0b40b2e147bcad1bd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/7b/2a/b411aaa219132a68b17937fc9431fd9eb9c23c12a7df3d134f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=8c32895950b6cc80f466b8d713be7ce4448fa46e335a8547ffbdb40b0dd7bc76\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "Successfully built params-flow py-params\n",
            "Installing collected packages: py-params, params-flow\n",
            "Successfully installed params-flow-0.7.1 py-params-0.9.7\n",
            "Collecting py-params==0.7.3\n",
            "  Downloading https://files.pythonhosted.org/packages/89/7b/b7d25e0f262599cab01ab7a3c1fd2d380566fc5bb4617fb725ddbbb8964d/py-params-0.7.3.tar.gz\n",
            "Building wheels for collected packages: py-params\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.7.3-cp36-none-any.whl size=4346 sha256=f7a18af75ff4a97ea226825003ef58b2ea645814a4628dc1ab94f949cf960d71\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/9f/a1/e7e79bd92eecef952a46b16d0bc93ffdc91d4b619f79777d27\n",
            "Successfully built py-params\n",
            "Installing collected packages: py-params\n",
            "  Found existing installation: py-params 0.9.7\n",
            "    Uninstalling py-params-0.9.7:\n",
            "      Successfully uninstalled py-params-0.9.7\n",
            "Successfully installed py-params-0.7.3\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Tensorflow version 2.2.0\n",
            "Running on TPU  ['10.88.227.98:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.98:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.88.227.98:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmM2Svrp24tN",
        "colab_type": "text"
      },
      "source": [
        "## Fix Directories for BERT\n",
        "First part of selecting which model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXaYCPNl_HQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EXPERIMENT = 'first_experiment'\n",
        "#CHECKPOINT_DIR = './checkpoints'\n",
        "\n",
        "# 'bert' or 'bert_large' or 'albert'\n",
        "MODEL = 'albert'\n",
        "ADDITIONAL_DATA = False\n",
        "identifier='test123'#for naming the output csv, weights and validation scores for graph\n",
        "\n",
        "# '' or '_monoise' or '_monoise_b'\n",
        "DATASET_PREPROCESSING = '' \n",
        "\n",
        "CHECKPOINT = None\n",
        "CHECKPOINT_CKPT = None\n",
        "CHECKPOINT_VOCAB = None\n",
        "CHECKPOINT_CONFIG = None\n",
        "\n",
        "if MODEL == 'bert':\n",
        "  CHECKPOINT = './bert/checkpoints/bert_base'\n",
        "if MODEL == 'bert_large':\n",
        "  CHECKPOINT = './bert/checkpoints/bert_large_wwm'\n",
        "if MODEL == 'albert':\n",
        "  CHECKPOINT = './bert/checkpoints/albert_xlarge'\n",
        "\n",
        "if MODEL == 'bert' or MODEL == 'bert_large':\n",
        "  CHECKPOINT_CKPT = os.path.join(CHECKPOINT, 'bert_model.ckpt')\n",
        "  CHECKPOINT_VOCAB = os.path.join(CHECKPOINT, 'vocab.txt')\n",
        "  CHECKPOINT_CONFIG = os.path.join(CHECKPOINT, 'bert_config.json')\n",
        "if MODEL == 'albert':\n",
        "  CHECKPOINT_VOCAB = os.path.join(CHECKPOINT, '30k-clean.model')\n",
        "  CHECKPOINT_CKPT = os.path.join(CHECKPOINT, 'model.ckpt-best')\n",
        "  CHECKPOINT_CONFIG = os.path.join(CHECKPOINT, 'albert_config.json')\n",
        "\n",
        "DATASET_DIR = './'\n",
        "DATASET_FILE_TRAIN_NEG = os.path.join(DATASET_DIR, 'twitter-datasets/train_neg_full{}.txt'.format(DATASET_PREPROCESSING))\n",
        "DATASET_FILE_TRAIN_POS = os.path.join(DATASET_DIR, 'twitter-datasets/train_pos_full{}.txt'.format(DATASET_PREPROCESSING))\n",
        "DATASET_FILE_TEST = os.path.join(DATASET_DIR, 'twitter-datasets/test_data{}.txt'.format(DATASET_PREPROCESSING))\n",
        "\n",
        "FILE_PATHS = [DATASET_FILE_TRAIN_POS, DATASET_FILE_TRAIN_NEG]\n",
        "\n",
        "#Learning parameters\n",
        "last_val_loss = None\n",
        "patience = 5\n",
        "no_improvement_since = 0\n",
        "current_learning_rate = 1e-5\n",
        "minimum_learning_rate = 1e-7\n",
        "reload_training = False\n",
        "reload_checkpoint = './twitter_bert_large_second.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43YtU-B29JL",
        "colab_type": "text"
      },
      "source": [
        "## methods to create BERT layer\n",
        "change name=\"albert\" for other model, have to adjust paths above too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXnr4jwJ_T1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_layers(root_layer):\n",
        "    if isinstance(root_layer, keras.layers.Layer):\n",
        "        yield root_layer\n",
        "    for layer in root_layer._layers:\n",
        "        for sub_layer in flatten_layers(layer):\n",
        "            yield sub_layer\n",
        "\n",
        "\n",
        "def freeze_bert_layers(l_bert):\n",
        "    \"\"\"\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
        "    \"\"\"\n",
        "    for layer in flatten_layers(l_bert):\n",
        "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
        "            layer.trainable = True\n",
        "        elif len(layer._layers) == 0:\n",
        "            layer.trainable = False\n",
        "        l_bert.embeddings_layer.trainable = False\n",
        "\n",
        "\n",
        "def create_learning_rate(learn_rate=5e-5):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        return float(learn_rate)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler\n",
        "\n",
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            #res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "            res = end_learn_rate\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler\n",
        "\n",
        "\n",
        "def create_model(max_seq_len, adapter_size=64):\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "    # adapter_size = 64  # see - arXiv:1902.00751\n",
        "\n",
        "    # create the bert layer\n",
        "    with tf.io.gfile.GFile(CHECKPOINT_CONFIG, \"r\") as reader:\n",
        "        #Download from Google\n",
        "        #model_dir    = bert.fetch_tfhub_albert_model(model_name, \".models\")\n",
        "        #bert_params = bert.albert_params(model_name)\n",
        "        #bert_l = BertModelLayer.from_params(bert_params, name=\"albert\", shared_layer=True, embedding_size=128)\n",
        "        \n",
        "        bc = StockBertConfig.from_json_string(reader.read())\n",
        "        bert_params = map_stock_config_to_params(bc)\n",
        "        bert_params.adapter_size = adapter_size\n",
        "        bert_l = None\n",
        "        if MODEL[:4] == 'bert':\n",
        "          bert_l = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "        else:\n",
        "          bert_l = BertModelLayer.from_params(bert_params, name=\"albert\", shared_layer=True, embedding_size=128)\n",
        "\n",
        "    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
        "    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
        "    # output         = bert([input_ids, token_type_ids])\n",
        "    output = bert_l(input_ids)\n",
        "\n",
        "    print(\"bert shape\", output.shape)\n",
        "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
        "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "    logits = keras.layers.Dropout(0.5)(logits)\n",
        "    logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
        "\n",
        "    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
        "    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "    model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "    # load the pre-trained model weights\n",
        "    if MODEL[:4] == 'bert':\n",
        "      load_stock_weights(bert_l, CHECKPOINT_CKPT)\n",
        "    else:\n",
        "      bert.load_albert_weights(bert_l, CHECKPOINT_CKPT)\n",
        "\n",
        "    # freeze weights if adapter-BERT is used\n",
        "    if adapter_size is not None:\n",
        "        freeze_bert_layers(bert_l)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(),\n",
        "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brlQdmnfqSUh",
        "colab_type": "text"
      },
      "source": [
        "# Define Dataloading Class\n",
        "Also loading data into array\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQiotBTjltcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = {}\n",
        "    data[\"sentence\"] = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data[\"sentence\"] = f.readlines()\n",
        "\n",
        "    #longest_string = max(data[\"sentence\"], key=len)\n",
        "    #print(longest_string)\n",
        "    #print(len(longest_string))\n",
        "\n",
        "    return pd.DataFrame.from_dict(data)\n",
        "\n",
        "def load_dataset(pos_directory, neg_directory):\n",
        "    pos_df = load_data(pos_directory)\n",
        "    neg_df = load_data(neg_directory)\n",
        "\n",
        "    pos_df[\"sentiment\"] = 1\n",
        "    neg_df[\"sentiment\"] = 0\n",
        "\n",
        "    return pd.concat([pos_df, neg_df])\n",
        "\n",
        "class MovieReviewData:\n",
        "    DATA_COLUMN = \"sentence\"\n",
        "    LABEL_COLUMN = \"sentiment\"\n",
        "\n",
        "    def __init__(self, tokenizer= FullTokenizer, max_seq_len=1024):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.epoch = None\n",
        "        self.sample_size= 128*937\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.train_x = None\n",
        "        self.train_x_token_types = None\n",
        "        self.test_x = None\n",
        "        self.test_x_token_types = None\n",
        "        self.train_y = None\n",
        "        self.test_y = None\n",
        "        \n",
        "    def _prepare(self, df):\n",
        "        x, y = [], []\n",
        "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
        "            for ndx, row in df.iterrows():\n",
        "                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "                x.append(token_ids)\n",
        "                y.append(int(label))\n",
        "                pbar.update()\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _pad(self, ids):\n",
        "        x, t = [], []\n",
        "        token_type_ids = [0] * self.max_seq_len\n",
        "        for input_ids in ids:\n",
        "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "            x.append(np.array(input_ids))\n",
        "            t.append(token_type_ids)\n",
        "        return np.array(x), np.array(t)\n",
        "\n",
        "    def load_data(self, epoch, update_test=True):\n",
        "        trainset = load_dataset(DATASET_FILE_TRAIN_POS, DATASET_FILE_TRAIN_NEG)\n",
        "        epoch = epoch % 200\n",
        "\n",
        "        trainset = shuffle(trainset, random_state=5)\n",
        "        #remove already processed data\n",
        "        train = trainset.head(2400000)\n",
        "        train = train.tail(200*128*93 - epoch*128*93)\n",
        "        train = train.head(128*93)\n",
        "        test = trainset.tail(100000)\n",
        "        #train = trainset.head(10*128)\n",
        "        #test = trainset.tail(10*128)\n",
        "\n",
        "        train = shuffle(train)\n",
        "        test = shuffle(test)\n",
        "\n",
        "        trainset.reset_index(inplace=True, drop=True)\n",
        "\n",
        "        if update_test:\n",
        "          ((self.train_x, self.train_y),\n",
        "          (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
        "\n",
        "          ((self.train_x, self.train_x_token_types),\n",
        "          (self.test_x, self.test_x_token_types)) = map(self._pad,\n",
        "                                                        [self.train_x, self.test_x])\n",
        "        else:\n",
        "          df_empty = pd.DataFrame({'empty' : []})\n",
        "          ((self.train_x, self.train_y),\n",
        "          (_, _)) = map(self._prepare, [train, df_empty])\n",
        "\n",
        "          ((self.train_x, self.train_x_token_types),\n",
        "          (_, _)) = map(self._pad, [self.train_x, []])\n",
        "\n",
        "spm_model = None\n",
        "sp = None\n",
        "\n",
        "if MODEL == 'albert':\n",
        "  spm_model = os.path.join('./bert/checkpoints/albert_xlarge/', \"30k-clean.model\")\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load(spm_model)\n",
        "\n",
        "tokenizer = None\n",
        "if MODEL[:4] == 'bert':\n",
        "  tokenizer = FullTokenizer(vocab_file=CHECKPOINT_VOCAB, do_lower_case=True)\n",
        "else:\n",
        "  tokenizer = AlbertFullTokenizer(vocab_file=None, do_lower_case=True, spm_model_file=CHECKPOINT_VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efXrHjE3CG4",
        "colab_type": "text"
      },
      "source": [
        "## BERT text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E4uy1eZCOgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process_positive(x, max_seq_len):\n",
        "  tokens = tokenizer.tokenize(x.numpy())\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n",
        "  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n",
        "  return token_ids, int(1)\n",
        "\n",
        "def pre_process_negative(x, max_seq_len):\n",
        "  tokens = tokenizer.tokenize(x.numpy())\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n",
        "  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n",
        "  return token_ids, int(0)\n",
        "\n",
        "def pre_process_text(x):\n",
        "  tokens = tokenizer.tokenize(x.numpy())\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  return tokens\n",
        "\n",
        "def dummy_pre_process(x):\n",
        "   return tf.constant(3, shape=(128,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsTc85ukInCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "309c15d6-a37d-41fa-dee1-14238beb2388"
      },
      "source": [
        "'''\n",
        "TOKENIZER = FullTokenizer(vocab_file='./bert/checkpoints/albert_large_v2/30k-clean.model', do_lower_case=True)\n",
        "\n",
        "spm_model = os.path.join('./bert/checkpoints/albert_large_v2/', \"30k-clean.model\")\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(spm_model)\n",
        "do_lower_case = True\n",
        "\n",
        "def pre_process_positive(x, max_seq_len):\n",
        "  processed_text = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  token_ids = bert.albert_tokenization.encode_ids(sp, processed_text) #sollte vom FullTokenizer erledigt werden können wie oben mit token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n",
        "  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n",
        "  return token_ids, int(1)\n",
        "\n",
        "def pre_process_negative(x, max_seq_len):\n",
        "  tokens = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  token_ids = bert.albert_tokenization.encode_ids(sp, processed_text)\n",
        "  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n",
        "  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n",
        "  return token_ids, int(0)\n",
        "\n",
        "def pre_process_text(x):\n",
        "\n",
        "  tokens = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\n",
        "  #tokens = TOKENIZER.tokenize(x.numpy())\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  return tokens\n",
        "\n",
        "def dummy_pre_process(x):\n",
        "   return tf.constant(3, shape=(128,))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTOKENIZER = FullTokenizer(vocab_file=\\'./bert/checkpoints/albert_large_v2/30k-clean.model\\', do_lower_case=True)\\n\\nspm_model = os.path.join(\\'./bert/checkpoints/albert_large_v2/\\', \"30k-clean.model\")\\nsp = spm.SentencePieceProcessor()\\nsp.load(spm_model)\\ndo_lower_case = True\\n\\ndef pre_process_positive(x, max_seq_len):\\n  processed_text = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\\n  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\\n  token_ids = bert.albert_tokenization.encode_ids(sp, processed_text)\\n  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\\n  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\\n  return token_ids, int(1)\\n\\ndef pre_process_negative(x, max_seq_len):\\n  tokens = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\\n  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\\n  token_ids = bert.albert_tokenization.encode_ids(sp, processed_text)\\n  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\\n  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\\n  return token_ids, int(0)\\n\\ndef pre_process_text(x):\\n\\n  tokens = bert.albert_tokenization.preprocess_text(x.numpy(), lower=do_lower_case)\\n  #tokens = TOKENIZER.tokenize(x.numpy())\\n  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\\n  return tokens\\n\\ndef dummy_pre_process(x):\\n   return tf.constant(3, shape=(128,))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR2PNyhBSa-5",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJfqna-xPbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "8df7d300-d3df-49f6-8128-fd97ee563447"
      },
      "source": [
        "max_seq_len = 128\n",
        "adapter_size = None\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    model = create_model(max_seq_len, adapter_size=adapter_size)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "total_epoch_count = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 128, 1024)\n",
            "Loading google-research/ALBERT weights...\n",
            "Done loading 22 BERT weights from: ./bert/checkpoints/albert_xlarge/model.ckpt-best into <bert.model.BertModelLayer object at 0x7ff381192d30> (prefix:albert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "\tglobal_step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "albert (BertModelLayer)      (None, 128, 1024)         16634112  \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 768)               787200    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1538      \n",
            "=================================================================\n",
            "Total params: 17,422,850\n",
            "Trainable params: 17,422,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idw95IXg3_yM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Train model/fit+save weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmI1kXiOxUYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "7bdde9b2-0afc-4559-8c31-adb538b1aa86"
      },
      "source": [
        "data = MovieReviewData(tokenizer, max_seq_len=128)\n",
        "\n",
        "val_losses = []\n",
        "val_accuracy = []\n",
        "train_losses = []\n",
        "train_accuracy = []\n",
        "\n",
        "if reload_training:\n",
        "  model.load_weights(reload_checkpoint) \n",
        "\n",
        "update_test = True\n",
        "for epoch in range(0,3):\n",
        "  print('Epoch {}'.format(epoch))\n",
        "  print('Loading Data...')\n",
        "  data.load_data(epoch, update_test)\n",
        "  if update_test:\n",
        "    update_test = False\n",
        "\n",
        "  dataset_train = tf.data.Dataset.from_tensor_slices((data.train_x, data.train_y))\n",
        "  dataset_train = dataset_train.batch(32, drop_remainder=True)\n",
        "\n",
        "  dataset_test = tf.data.Dataset.from_tensor_slices((data.test_x, data.test_y))\n",
        "  dataset_test = dataset_test.batch(32, drop_remainder=True)\n",
        "\n",
        "  train_history = model.fit(dataset_train,\n",
        "                  epochs=1,\n",
        "                  callbacks=[create_learning_rate(learn_rate=current_learning_rate)])\n",
        "  \n",
        "  loss = train_history.history['loss'][0]\n",
        "  accuracy = train_history.history['acc'][0]\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_accuracy.append(accuracy)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print('Run Validation...')\n",
        "    results = model.evaluate(dataset_test)\n",
        "    val_loss = results[0]\n",
        "    val_acc = results[1]\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracy.append(val_acc)\n",
        "\n",
        "    if last_val_loss is None or last_val_loss > val_loss:\n",
        "      last_val_loss = val_loss\n",
        "      no_improvement_since = 0\n",
        "    else:\n",
        "      no_improvement_since += 1\n",
        "      if no_improvement_since >= patience:\n",
        "        current_learning_rate = current_learning_rate * 0.1\n",
        "        if current_learning_rate < minimum_learning_rate:\n",
        "          print('Learning Finished Early')\n",
        "          break\n",
        "        print('Decreasing Learning Rate to: {}'.format(current_learning_rate))\n",
        "      else:\n",
        "        print('No improvement since: {}'.format(no_improvement_since))\n",
        "\n",
        "\n",
        "  if epoch != 0 and epoch % 100 == 0:\n",
        "    model.save_weights('./'+MODEL+identifier+'_epoch_{}.h5'.format(epoch), overwrite=True)\n",
        "\n",
        "model.save_weights('./'+MODEL+identifier+'.h5', overwrite=True)\n",
        "\n",
        "# Write Results\n",
        "if not os.path.exists('./results'):\n",
        "  os.mkdir('./results')\n",
        "\n",
        "f = open('./results/'+MODEL+identifier+'_test.txt', 'w')\n",
        "for x in range(len(train_losses)):\n",
        "  f.write('{} {} {}\\n'.format(x, train_losses[x], train_accuracy[x]))\n",
        "f.close()\n",
        "\n",
        "f = open('./results/'+MODEL+identifier+'_validation.txt', 'w')\n",
        "for x in range(len(val_losses)):\n",
        "  f.write('{} {} {}\\n'.format(x * 100, val_losses[x], val_accuracy[x]))\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Loading Data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11.9k/11.9k [00:02<00:00, 4.18kit/s]\n",
            "100%|██████████| 100k/100k [00:22<00:00, 4.38kit/s] \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "372/372 [==============================] - 30s 80ms/step - acc: 0.5376 - loss: 0.7137 - lr: 1.0000e-05\n",
            "Run Validation...\n",
            "3125/3125 [==============================] - 91s 29ms/step - acc: 0.5154 - loss: 0.7035\n",
            "Epoch 1\n",
            "Loading Data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11.9k/11.9k [00:02<00:00, 4.51kit/s]\n",
            "0.00it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "372/372 [==============================] - 30s 80ms/step - acc: 0.5510 - loss: 0.7060 - lr: 1.0000e-05\n",
            "Epoch 2\n",
            "Loading Data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11.9k/11.9k [00:02<00:00, 4.39kit/s]\n",
            "0.00it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "372/372 [==============================] - 30s 80ms/step - acc: 0.5314 - loss: 0.7135 - lr: 1.0000e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L24eTyrfeyJM",
        "colab_type": "text"
      },
      "source": [
        "## Generate Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmq0A3GXdm-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to generate Plots from Files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUCypRRM4RjF",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess test-set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWS4vv1VE87w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b8302d3-7291-460d-e0a1-a4316eba3de9"
      },
      "source": [
        "positive_data = []\n",
        "with open(DATASET_FILE_TRAIN_POS, \"r\") as f:\n",
        "  for x in f:\n",
        "    positive_data.append(x)\n",
        "\n",
        "negative_data = []\n",
        "with open(DATASET_FILE_TRAIN_NEG, \"r\") as f:\n",
        "  for x in f:\n",
        "    negative_data.append(x)\n",
        "\n",
        "\n",
        "print(len(positive_data))\n",
        "positive_data = list(dict.fromkeys(positive_data))\n",
        "print(len(positive_data))\n",
        "\n",
        "print(len(negative_data))\n",
        "negative_data = list(dict.fromkeys(negative_data))\n",
        "print(len(negative_data))\n",
        "test_data = []\n",
        "with open(DATASET_FILE_TEST, \"r\") as f:\n",
        "  for x in f:\n",
        "    test_data.append(x)\n",
        "\n",
        "encoded_test_data = []\n",
        "for x in test_data:\n",
        "  result, _ = pre_process_positive(tf.convert_to_tensor(x), 128)\n",
        "  encoded_test_data.append(tf.reshape(tf.convert_to_tensor(result), (1,128)))\n",
        "encoded_test_ds = tf.data.Dataset.from_tensor_slices(encoded_test_data)\n",
        "for x in encoded_test_ds.take(5):\n",
        "  print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1250000\n",
            "1127644\n",
            "1250000\n",
            "1142838\n",
            "tf.Tensor(\n",
            "[[    2   137    15  6858    13 12473   895   598 27744    13     5  1059\n",
            "     29    14 13469   598     8 12473   598  8328   111   815 19863   282\n",
            "     13    15  1179  1045    19    14   308    17    13     9     9     9\n",
            "     13     1   911   255     1     3     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[    2   172    15     1 16704     1  5728  2601    18   134    31   170\n",
            "     65   877    86   130    31    92    22    38   340 10497    42    27\n",
            "     13   187  1962    17   442   273 10769    19   154 28539    13   187\n",
            "     13   187    13   187     3     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[   2  203   15   49 2973 1179  229   37 6256   30   18   51 1578    3\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]], shape=(1, 128), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[    2   268    15     1 16704     1    90  1216    22   765    13   187\n",
            "     13   187    13   187    13 20248   797  5759  1123    17    52  1065\n",
            "   8628  5907  3375    13 19073   111     3     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[    2   331    15  3185  5221    31  1080  5718  1953    14   983    13\n",
            "     15    31   550  3290    71    29    21 14276     3     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4k1CZtJ4pS8",
        "colab_type": "text"
      },
      "source": [
        "## Predict test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oauQeQCvJ2Qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(encoded_test_ds)\n",
        "labels = np.argmax(results, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE2UiBBn4x2X",
        "colab_type": "text"
      },
      "source": [
        "## Postprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNIjYFDKBU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labels = [1 if x == 1 else -1 for x in labels]\n",
        "\n",
        "#reverse labels\n",
        "labels = [-1 if x == 1 else 1 for x in labels]\n",
        "#un-reverse labels\n",
        "labels = [1 if x == -1 else -1 for x in labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKSwMczRZCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'Id': np.arange(1,10001),\n",
        "                  'Prediction': labels\n",
        "                   })\n",
        "df.to_csv(MODEL+identifier+'.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}