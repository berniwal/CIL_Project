{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIL-Notebook.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/berniwal/CIL_Project/blob/master/CIL-Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"e3SIsoqF2sKj","colab_type":"text"},"source":["## Go to correct directory"]},{"cell_type":"code","metadata":{"id":"3IzoDHLY-ykE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1592376885493,"user_tz":-120,"elapsed":28944,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"8d541710-83d2-4eab-a0a3-e3fdbd2c8d62"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.getcwd()\n","#os.chdir('/content/drive/My Drive/CIL')\n","os.chdir('/content/drive/My Drive/CIL/CIL_Project/CIL')\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/CIL/CIL_Project/CIL'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"RXAkBoig2yy-","colab_type":"text"},"source":["## install + import libraries"]},{"cell_type":"code","metadata":{"id":"Ep6izaAW_JwK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592376908959,"user_tz":-120,"elapsed":52388,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"0f547bb4-e8e1-491c-f88c-edf7bf551cdf"},"source":["!pip install params_flow==0.7.1\n","!pip install py-params==0.7.3\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import os\n","from datetime import datetime\n","\n","import bert\n","from bert.tokenization.bert_tokenization import FullTokenizer\n","from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n","from bert import BertModelLayer\n","\n","from tqdm import tqdm\n","import numpy as np\n","\n","from sklearn.utils import shuffle"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting params_flow==0.7.1\n","  Downloading https://files.pythonhosted.org/packages/81/ae/c286650d720d3b3f7e6c758d5af9ab62dc5b4c0d1c3d910baccb1be6f70f/params-flow-0.7.1.tar.gz\n","Collecting py-params>=0.6.4\n","  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params_flow==0.7.1) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params_flow==0.7.1) (4.41.1)\n","Building wheels for collected packages: params-flow, py-params\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.7.1-cp36-none-any.whl size=15376 sha256=aa39d32b25e284f3c8a99ab70f0fec122273addfe2a54848581f84de2d890afe\n","  Stored in directory: /root/.cache/pip/wheels/e2/7b/2a/b411aaa219132a68b17937fc9431fd9eb9c23c12a7df3d134f\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=55d0a40bb2b3a2809bf9661b6895bfa848a0f943cfeb2bd2ce96d578a0b23180\n","  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n","Successfully built params-flow py-params\n","Installing collected packages: py-params, params-flow\n","Successfully installed params-flow-0.7.1 py-params-0.9.7\n","Collecting py-params==0.7.3\n","  Downloading https://files.pythonhosted.org/packages/89/7b/b7d25e0f262599cab01ab7a3c1fd2d380566fc5bb4617fb725ddbbb8964d/py-params-0.7.3.tar.gz\n","Building wheels for collected packages: py-params\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.7.3-cp36-none-any.whl size=4346 sha256=bdc6197225542e9bcdc1cd4163ce2a395440631ed6842238bd00f2fa362e7122\n","  Stored in directory: /root/.cache/pip/wheels/a3/9f/a1/e7e79bd92eecef952a46b16d0bc93ffdc91d4b619f79777d27\n","Successfully built py-params\n","Installing collected packages: py-params\n","  Found existing installation: py-params 0.9.7\n","    Uninstalling py-params-0.9.7:\n","      Successfully uninstalled py-params-0.9.7\n","Successfully installed py-params-0.7.3\n","Tensorflow version 2.2.0\n","Running on TPU  ['10.56.3.186:8470']\n","INFO:tensorflow:Initializing the TPU system: grpc://10.56.3.186:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.56.3.186:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FmM2Svrp24tN","colab_type":"text"},"source":["## Fix Directories for BERT\n","First part of selecting which model"]},{"cell_type":"code","metadata":{"id":"AXaYCPNl_HQh","colab_type":"code","colab":{}},"source":["EXPERIMENT = 'first_experiment'\n","\n","CHECKPOINT_DIR = './checkpoints'\n","\n","CHECKPOINT = './bert/checkpoints/bert_base'\n","#CHECKPOINT = './bert/checkpoints/bert_large_wwm'\n","\n","CHECKPOINT_CKPT = os.path.join(CHECKPOINT, 'bert_model.ckpt')\n","CHECKPOINT_VOCAB = os.path.join(CHECKPOINT, 'vocab.txt')\n","CHECKPOINT_CONFIG = os.path.join(CHECKPOINT, 'bert_config.json')\n","\n","DATASET_DIR = './'\n","\n","DATASET_FILE_TRAIN_NEG = os.path.join(DATASET_DIR, 'twitter-datasets/train_neg_full.txt')\n","DATASET_FILE_TRAIN_POS = os.path.join(DATASET_DIR, 'twitter-datasets/train_pos_full.txt')\n","DATASET_FILE_TEST = os.path.join(DATASET_DIR, 'twitter-datasets/test_data.txt')\n","\n","DATASET_FILE_TRAIN_NEG_EXTRA = os.path.join(DATASET_DIR, 'extra_data/preprocessed_neg.txt')\n","DATASET_FILE_TRAIN_POS_EXTRA = os.path.join(DATASET_DIR, 'extra_data/preprocessed_pos.txt')\n","\n","FILE_PATHS = [DATASET_FILE_TRAIN_POS, DATASET_FILE_TRAIN_NEG]\n","\n","#402, 324\n","MAX_SEQ_LENGTH = 128\n","LEARNING_RATE = 2e-5\n","BATCH_SIZE = 16\n","NUM_TRAIN_EPOCHS = 3.0\n","WARMUP_PROPORTION = 0.1\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","\n","label_list = [0, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E43YtU-B29JL","colab_type":"text"},"source":["## methods to create BERT layer\n","change name=\"albert\" for other model, have to adjust paths above too"]},{"cell_type":"code","metadata":{"id":"lXnr4jwJ_T1D","colab_type":"code","colab":{}},"source":["def flatten_layers(root_layer):\n","    if isinstance(root_layer, keras.layers.Layer):\n","        yield root_layer\n","    for layer in root_layer._layers:\n","        for sub_layer in flatten_layers(layer):\n","            yield sub_layer\n","\n","\n","def freeze_bert_layers(l_bert):\n","    \"\"\"\n","    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n","    \"\"\"\n","    for layer in flatten_layers(l_bert):\n","        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n","            layer.trainable = True\n","        elif len(layer._layers) == 0:\n","            layer.trainable = False\n","        l_bert.embeddings_layer.trainable = False\n","\n","\n","def create_learning_rate_scheduler(max_learn_rate=5e-5,\n","                                   end_learn_rate=1e-5,\n","                                   warmup_epoch_count=10,\n","                                   total_epoch_count=90):\n","\n","    def lr_scheduler(epoch):\n","        if epoch < warmup_epoch_count:\n","            #res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n","            res = end_learn_rate\n","        else:\n","            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n","        return float(res)\n","    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n","\n","    return learning_rate_scheduler\n","\n","\n","def create_model(max_seq_len, adapter_size=64):\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    # adapter_size = 64  # see - arXiv:1902.00751\n","\n","    # create the bert layer\n","    with tf.io.gfile.GFile(CHECKPOINT_CONFIG, \"r\") as reader:\n","        bc = StockBertConfig.from_json_string(reader.read())\n","        bert_params = map_stock_config_to_params(bc)\n","        bert_params.adapter_size = adapter_size\n","        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n","\n","    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n","    # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n","    # output         = bert([input_ids, token_type_ids])\n","    output = bert(input_ids)\n","\n","    print(\"bert shape\", output.shape)\n","    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n","    cls_out = keras.layers.Dropout(0.5)(cls_out)\n","    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n","    logits = keras.layers.Dropout(0.5)(logits)\n","    logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n","\n","    # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n","    # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n","    model = keras.Model(inputs=input_ids, outputs=logits)\n","    model.build(input_shape=(None, max_seq_len))\n","\n","    # load the pre-trained model weights\n","    load_stock_weights(bert, CHECKPOINT_CKPT)\n","\n","    # freeze weights if adapter-BERT is used\n","    if adapter_size is not None:\n","        freeze_bert_layers(bert)\n","\n","    model.compile(optimizer=keras.optimizers.Adam(),\n","                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1efXrHjE3CG4","colab_type":"text"},"source":["## BERT text preprocessing"]},{"cell_type":"code","metadata":{"id":"0E4uy1eZCOgS","colab_type":"code","colab":{}},"source":["TOKENIZER = FullTokenizer(vocab_file=CHECKPOINT_VOCAB, do_lower_case=True)\n","\n","def pre_process_positive(x, max_seq_len):\n","  tokens = TOKENIZER.tokenize(x.numpy())\n","  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","  token_ids = TOKENIZER.convert_tokens_to_ids(tokens)\n","  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n","  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n","  return token_ids, int(1)\n","\n","def pre_process_negative(x, max_seq_len):\n","  tokens = TOKENIZER.tokenize(x.numpy())\n","  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","  token_ids = TOKENIZER.convert_tokens_to_ids(tokens)\n","  token_ids = token_ids[:min(len(token_ids), max_seq_len - 2)]\n","  token_ids = np.concatenate((token_ids, np.zeros((max_seq_len - len(token_ids))))).astype(np.int32)\n","  return token_ids, int(0)\n","\n","def pre_process_text(x):\n","  tokens = TOKENIZER.tokenize(x.numpy())\n","  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","  return tokens\n","\n","def dummy_pre_process(x):\n","   return tf.constant(3, shape=(128,))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dMPXgnrLCrDG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592376909934,"user_tz":-120,"elapsed":53327,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"99b2fa98-da76-4fb3-f63a-3a89c1b38137"},"source":["'''positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\n","\n","f = open(\"./twitter-datasets/preprocessed_positive.txt\", \"w\")\n","\n","import tensorflow_datasets as tfds\n","\n","ENCODER = tfds.features.text.SubwordTextEncoder(\n","    vocab_list=TOKENIZER.vocab\n",")\n","\n","for x in positive:\n","  write = pre_process_text(x)\n","  write = ''.join(map(str, write))\n","  f.write(write + '\\n')\n","\n","f.close()'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\\n\\nf = open(\"./twitter-datasets/preprocessed_positive.txt\", \"w\")\\n\\nimport tensorflow_datasets as tfds\\n\\nENCODER = tfds.features.text.SubwordTextEncoder(\\n    vocab_list=TOKENIZER.vocab\\n)\\n\\nfor x in positive:\\n  write = pre_process_text(x)\\n  write = \\'\\'.join(map(str, write))\\n  f.write(write + \\'\\n\\')\\n\\nf.close()'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"9Z0jwz99DFGG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592376909935,"user_tz":-120,"elapsed":53309,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"939bd5de-290b-41a3-ae22-59a79560dc20"},"source":["'''positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\n","negative = tf.data.TextLineDataset(DATASET_FILE_TRAIN_NEG)\n","\n","positive = positive.map(\n","    lambda x: tf.py_function(pre_process_positive, [x,128], (tf.int32, tf.int32)),\n","    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","negative = negative.map(\n","    lambda x: tf.py_function(pre_process_negative, [x, 128], (tf.int32, tf.int32)),\n","    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","dataset = tf.data.experimental.sample_from_datasets([positive,negative]).batch(32)'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\\nnegative = tf.data.TextLineDataset(DATASET_FILE_TRAIN_NEG)\\n\\npositive = positive.map(\\n    lambda x: tf.py_function(pre_process_positive, [x,128], (tf.int32, tf.int32)),\\n    num_parallel_calls=tf.data.experimental.AUTOTUNE)\\nnegative = negative.map(\\n    lambda x: tf.py_function(pre_process_negative, [x, 128], (tf.int32, tf.int32)),\\n    num_parallel_calls=tf.data.experimental.AUTOTUNE)\\n\\ndataset = tf.data.experimental.sample_from_datasets([positive,negative]).batch(32)'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"5YtEGHuAFOti","colab_type":"text"},"source":["# Store Data in TF-Record Files, such that we don't get Memory Issues and have a valid Input Pipeline."]},{"cell_type":"code","metadata":{"id":"L1FFaldgvoHK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592376909936,"user_tz":-120,"elapsed":53288,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"48590ec1-8609-4b49-ecf2-0aa02233f118"},"source":["'''positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\n","negative = tf.data.TextLineDataset(DATASET_FILE_TRAIN_NEG)\n","file_name = './twitter-datasets/tfrecord_files/positive.tfrecord'\n","\n","with tf.io.TFRecordWriter(file_name) as writer: \n","  for idx, x in tqdm(enumerate(positive)):\n","    sentence, label = pre_process_positive(x, 128)\n","\n","    feature = {\n","        \"sentence\": tf.train.Feature(int64_list=tf.train.Int64List(value=sentence)),\n","        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n","    }\n","\n","    proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","    sample = proto.SerializeToString()\n","\n","    writer.write(sample)\n","\n","file_name_negative = './twitter-datasets/tfrecord_files/negative.tfrecord'\n","\n","with tf.io.TFRecordWriter(file_name_negative) as writer: \n","  for idx, x in tqdm(enumerate(negative)):\n","    sentence, label = pre_process_negative(x, 128)\n","\n","    feature = {\n","        \"sentence\": tf.train.Feature(int64_list=tf.train.Int64List(value=sentence)),\n","        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n","    }\n","\n","    proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","    sample = proto.SerializeToString()\n","\n","    writer.write(sample)'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'positive = tf.data.TextLineDataset(DATASET_FILE_TRAIN_POS)\\nnegative = tf.data.TextLineDataset(DATASET_FILE_TRAIN_NEG)\\nfile_name = \\'./twitter-datasets/tfrecord_files/positive.tfrecord\\'\\n\\nwith tf.io.TFRecordWriter(file_name) as writer: \\n  for idx, x in tqdm(enumerate(positive)):\\n    sentence, label = pre_process_positive(x, 128)\\n\\n    feature = {\\n        \"sentence\": tf.train.Feature(int64_list=tf.train.Int64List(value=sentence)),\\n        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\\n    }\\n\\n    proto = tf.train.Example(features=tf.train.Features(feature=feature))\\n    sample = proto.SerializeToString()\\n\\n    writer.write(sample)\\n\\nfile_name_negative = \\'./twitter-datasets/tfrecord_files/negative.tfrecord\\'\\n\\nwith tf.io.TFRecordWriter(file_name_negative) as writer: \\n  for idx, x in tqdm(enumerate(negative)):\\n    sentence, label = pre_process_negative(x, 128)\\n\\n    feature = {\\n        \"sentence\": tf.train.Feature(int64_list=tf.train.Int64List(value=sentence)),\\n        \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\\n    }\\n\\n    proto = tf.train.Example(features=tf.train.Features(feature=feature))\\n    sample = proto.SerializeToString()\\n\\n    writer.write(sample)'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"brlQdmnfqSUh","colab_type":"text"},"source":["# First 1 Mio\n","Also loading data into array\n"]},{"cell_type":"code","metadata":{"id":"fQiotBTjltcC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592377552847,"user_tz":-120,"elapsed":696180,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"971bfc5a-2a09-4061-a849-3ed57ba49ee0"},"source":["def load_data(file_path):\n","    data = {}\n","    data[\"sentence\"] = []\n","    with open(file_path, \"r\") as f:\n","        data[\"sentence\"] = f.readlines()\n","\n","    #longest_string = max(data[\"sentence\"], key=len)\n","    #print(longest_string)\n","    #print(len(longest_string))\n","\n","    return pd.DataFrame.from_dict(data)\n","\n","def load_dataset(pos_directory, neg_directory):\n","    pos_df = load_data(pos_directory)\n","    neg_df = load_data(neg_directory)\n","\n","    pos_df[\"sentiment\"] = 1\n","    neg_df[\"sentiment\"] = 0\n","\n","    return pd.concat([pos_df, neg_df])\n","\n","class MovieReviewData:\n","    DATA_COLUMN = \"sentence\"\n","    LABEL_COLUMN = \"sentiment\"\n","\n","    def __init__(self, tokenizer= FullTokenizer, sample_size=None, max_seq_len=1024):\n","        self.tokenizer = tokenizer\n","        self.sample_size = sample_size\n","        self.max_seq_len = 0\n","        trainset = load_dataset(DATASET_FILE_TRAIN_POS, DATASET_FILE_TRAIN_NEG)\n","\n","        trainset = shuffle(trainset, random_state=5)\n","\n","        train = trainset.head(2250000)\n","        test = trainset.tail(250000)\n","\n","        train = shuffle(train)\n","        test = shuffle(test)\n","\n","        trainset.reset_index(inplace=True, drop=True)\n","\n","        #train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index),\n","        #                 [train, test])\n","\n","        if sample_size is not None:\n","            assert sample_size % 128 == 0\n","            train, test = train.head(sample_size), test.head(250000)\n","            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n","\n","        ((self.train_x, self.train_y),\n","         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n","\n","        print(\"max seq_len\", self.max_seq_len)\n","        self.max_seq_len = max_seq_len\n","        #self.max_seq_len = min(self.max_seq_len, max_seq_len)\n","        ((self.train_x, self.train_x_token_types),\n","         (self.test_x, self.test_x_token_types)) = map(self._pad,\n","                                                       [self.train_x, self.test_x])\n","\n","    def _prepare(self, df):\n","        x, y = [], []\n","        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n","            for ndx, row in df.iterrows():\n","                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n","                tokens = self.tokenizer.tokenize(text)\n","                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n","                x.append(token_ids)\n","                y.append(int(label))\n","                pbar.update()\n","        return np.array(x), np.array(y)\n","\n","    def _pad(self, ids):\n","        x, t = [], []\n","        token_type_ids = [0] * self.max_seq_len\n","        for input_ids in ids:\n","            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n","            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n","            x.append(np.array(input_ids))\n","            t.append(token_type_ids)\n","        return np.array(x), np.array(t)\n","\n","tokenizer = FullTokenizer(vocab_file=CHECKPOINT_VOCAB, do_lower_case=True)\n","data = MovieReviewData(tokenizer, sample_size=10*128*878, max_seq_len=128)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1.12M/1.12M [08:03<00:00, 2.32kit/s]\n","100%|██████████| 250k/250k [01:46<00:00, 2.34kit/s]\n"],"name":"stderr"},{"output_type":"stream","text":["max seq_len 135\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EIOT7_Ivqux-","colab_type":"text"},"source":["# Second Mio (not needed for first tests)\n"]},{"cell_type":"code","metadata":{"id":"fDOlzTZGqga1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592378190970,"user_tz":-120,"elapsed":1334282,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"83581cf4-2b59-4a1c-92db-8444c6aa6b1f"},"source":["\n","def load_data(file_path):\n","    data = {}\n","    data[\"sentence\"] = []\n","    with open(file_path, \"r\") as f:\n","        data[\"sentence\"] = f.readlines()\n","\n","    #longest_string = max(data[\"sentence\"], key=len)\n","    #print(longest_string)\n","    #print(len(longest_string))\n","\n","    return pd.DataFrame.from_dict(data)\n","\n","def load_dataset(pos_directory, neg_directory):\n","    pos_df = load_data(pos_directory)\n","    neg_df = load_data(neg_directory)\n","\n","    pos_df[\"sentiment\"] = 1\n","    neg_df[\"sentiment\"] = 0\n","\n","    return pd.concat([pos_df, neg_df])\n","\n","class MovieReviewData:\n","    DATA_COLUMN = \"sentence\"\n","    LABEL_COLUMN = \"sentiment\"\n","\n","    def __init__(self, tokenizer= FullTokenizer, sample_size=None, max_seq_len=1024):\n","        self.tokenizer = tokenizer\n","        self.sample_size = sample_size\n","        self.max_seq_len = 0\n","        trainset = load_dataset(DATASET_FILE_TRAIN_POS, DATASET_FILE_TRAIN_NEG)\n","\n","        trainset = shuffle(trainset, random_state=5)\n","\n","        train = trainset.tail(1375000)\n","        train = train.head(1125000)\n","        test = trainset.tail(250000)\n","\n","        train = shuffle(train)\n","        test = shuffle(test)\n","\n","        trainset.reset_index(inplace=True, drop=True)\n","\n","        #train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index),\n","        #                 [train, test])\n","\n","        if sample_size is not None:\n","            assert sample_size % 128 == 0\n","            train, test = train.head(sample_size), test.head(250000)\n","            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n","\n","        ((self.train_x, self.train_y),\n","         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n","\n","        print(\"max seq_len\", self.max_seq_len)\n","        self.max_seq_len = max_seq_len\n","        #self.max_seq_len = min(self.max_seq_len, max_seq_len)\n","        ((self.train_x, self.train_x_token_types),\n","         (self.test_x, self.test_x_token_types)) = map(self._pad,\n","                                                       [self.train_x, self.test_x])\n","\n","    def _prepare(self, df):\n","        x, y = [], []\n","        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n","            for ndx, row in df.iterrows():\n","                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n","                tokens = self.tokenizer.tokenize(text)\n","                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n","                x.append(token_ids)\n","                y.append(int(label))\n","                pbar.update()\n","        return np.array(x), np.array(y)\n","\n","    def _pad(self, ids):\n","        x, t = [], []\n","        token_type_ids = [0] * self.max_seq_len\n","        for input_ids in ids:\n","            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n","            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n","            x.append(np.array(input_ids))\n","            t.append(token_type_ids)\n","        return np.array(x), np.array(t)\n","\n","tokenizer = FullTokenizer(vocab_file=CHECKPOINT_VOCAB, do_lower_case=True)\n","data = MovieReviewData(tokenizer, sample_size=10*128*878, max_seq_len=128)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1.12M/1.12M [08:02<00:00, 2.33kit/s]\n","100%|██████████| 250k/250k [01:46<00:00, 2.35kit/s]\n"],"name":"stderr"},{"output_type":"stream","text":["max seq_len 116\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZAKpCQXJj1Gl","colab_type":"text"},"source":["# Extra Data"]},{"cell_type":"code","metadata":{"id":"kLmVd0wPj0IO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592378805605,"user_tz":-120,"elapsed":1948892,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"8f60a068-01d0-444d-94ca-009d16398719"},"source":["def load_data(file_path):\n","    data = {}\n","    data[\"sentence\"] = []\n","    with open(file_path, \"r\") as f:\n","        data[\"sentence\"] = f.readlines()\n","\n","    #longest_string = max(data[\"sentence\"], key=len)\n","    #print(longest_string)\n","    #print(len(longest_string))\n","\n","    return pd.DataFrame.from_dict(data)\n","\n","def load_dataset(pos_directory, neg_directory):\n","    pos_df = load_data(pos_directory)\n","    neg_df = load_data(neg_directory)\n","\n","    pos_df[\"sentiment\"] = 1\n","    neg_df[\"sentiment\"] = 0\n","\n","    return pd.concat([pos_df, neg_df])\n","\n","class MovieReviewData:\n","    DATA_COLUMN = \"sentence\"\n","    LABEL_COLUMN = \"sentiment\"\n","\n","    def __init__(self, tokenizer= FullTokenizer, sample_size=None, max_seq_len=1024):\n","        self.tokenizer = tokenizer\n","        self.sample_size = sample_size\n","        self.max_seq_len = 0\n","        trainset = load_dataset(DATASET_FILE_TRAIN_POS_EXTRA, DATASET_FILE_TRAIN_NEG_EXTRA)\n","\n","        trainset = shuffle(trainset, random_state=5)\n","\n","        train = trainset.head(2250000)\n","        test = trainset.tail(250000)\n","\n","        train = shuffle(train)\n","        test = shuffle(test)\n","\n","        trainset.reset_index(inplace=True, drop=True)\n","\n","        #train, test = map(lambda df: df.reindex(df[MovieReviewData.DATA_COLUMN].str.len().sort_values().index),\n","        #                 [train, test])\n","\n","        if sample_size is not None:\n","            assert sample_size % 128 == 0\n","            train, test = train.head(sample_size), test.head(250000)\n","            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n","\n","        ((self.train_x, self.train_y),\n","         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n","\n","        print(\"max seq_len\", self.max_seq_len)\n","        self.max_seq_len = max_seq_len\n","        #self.max_seq_len = min(self.max_seq_len, max_seq_len)\n","        ((self.train_x, self.train_x_token_types),\n","         (self.test_x, self.test_x_token_types)) = map(self._pad,\n","                                                       [self.train_x, self.test_x])\n","\n","    def _prepare(self, df):\n","        x, y = [], []\n","        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n","            for ndx, row in df.iterrows():\n","                text, label = row[MovieReviewData.DATA_COLUMN], row[MovieReviewData.LABEL_COLUMN]\n","                tokens = self.tokenizer.tokenize(text)\n","                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n","                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n","                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n","                x.append(token_ids)\n","                y.append(int(label))\n","                pbar.update()\n","        return np.array(x), np.array(y)\n","\n","    def _pad(self, ids):\n","        x, t = [], []\n","        token_type_ids = [0] * self.max_seq_len\n","        for input_ids in ids:\n","            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n","            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n","            x.append(np.array(input_ids))\n","            t.append(token_type_ids)\n","        return np.array(x), np.array(t)\n","\n","tokenizer = FullTokenizer(vocab_file=CHECKPOINT_VOCAB, do_lower_case=True)\n","data = MovieReviewData(tokenizer, sample_size=10*128*878, max_seq_len=128)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1.12M/1.12M [07:43<00:00, 2.43kit/s]\n","100%|██████████| 250k/250k [01:41<00:00, 2.46kit/s]\n"],"name":"stderr"},{"output_type":"stream","text":["max seq_len 399\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dsu2HDD1tk24","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592378805610,"user_tz":-120,"elapsed":1948880,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"e0f0228f-05de-4a19-d1a8-3bf012f0228c"},"source":["'''def tfrecord_to_dataset(filenames):\n","  ds = tf.data.TFRecordDataset(filenames)\n","  feature_description = {\n","      \"sentence\":  tf.io.VarLenFeature(tf.int64),\n","      \"label\":      tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n","  }\n","\n","  def parse_proto(proto):\n","    example = tf.io.parse_single_example(proto, feature_description)\n","    token_ids, label = example[\"sentence\"], example[\"label\"]\n","    token_ids = tf.sparse.to_dense(token_ids)\n","    return token_ids, label\n","\n","  return ds.map(parse_proto)'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def tfrecord_to_dataset(filenames):\\n  ds = tf.data.TFRecordDataset(filenames)\\n  feature_description = {\\n      \"sentence\":  tf.io.VarLenFeature(tf.int64),\\n      \"label\":      tf.io.FixedLenFeature([], tf.int64, default_value=-1)\\n  }\\n\\n  def parse_proto(proto):\\n    example = tf.io.parse_single_example(proto, feature_description)\\n    token_ids, label = example[\"sentence\"], example[\"label\"]\\n    token_ids = tf.sparse.to_dense(token_ids)\\n    return token_ids, label\\n\\n  return ds.map(parse_proto)'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"Xsnvc2Ti0sLi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592378805613,"user_tz":-120,"elapsed":1948862,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"5faab980-0011-4f3c-ef1c-c2cb225aee9e"},"source":["'''file_name = './twitter-datasets/tfrecord_files/positive.tfrecord'\n","file_name_negative = './twitter-datasets/tfrecord_files/negative.tfrecord'\n","\n","ds = tfrecord_to_dataset([file_name, file_name_negative])\n","ds = ds.shuffle(buffer_size=2500000, seed=1000)\n","ds = ds.batch(32, drop_remainder = True)\n","\n","for x in ds.take(1):\n","  print(x)'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"file_name = './twitter-datasets/tfrecord_files/positive.tfrecord'\\nfile_name_negative = './twitter-datasets/tfrecord_files/negative.tfrecord'\\n\\nds = tfrecord_to_dataset([file_name, file_name_negative])\\nds = ds.shuffle(buffer_size=2500000, seed=1000)\\nds = ds.batch(32, drop_remainder = True)\\n\\nfor x in ds.take(1):\\n  print(x)\""]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"rckIyAA33VGP","colab_type":"text"},"source":["## data from array to tensor"]},{"cell_type":"code","metadata":{"id":"BBML0pLdzc1E","colab_type":"code","colab":{}},"source":["dataset_train = tf.data.Dataset.from_tensor_slices((data.train_x, data.train_y))\n","dataset_train = dataset_train.batch(32, drop_remainder=True)\n","\n","dataset_test = tf.data.Dataset.from_tensor_slices((data.test_x, data.test_y))\n","dataset_test = dataset_test.batch(32, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5P7HjXV_30t-","colab_type":"code","colab":{}},"source":["## Build model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UaJfqna-xPbs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":632},"executionInfo":{"status":"ok","timestamp":1592378846464,"user_tz":-120,"elapsed":1989681,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"67eb230c-6de0-45a1-b91e-1fe3ea59716d"},"source":["max_seq_len = 128\n","adapter_size = None\n","\n","with tpu_strategy.scope():\n","    model = create_model(max_seq_len, adapter_size=adapter_size)\n","\n","#model = create_model(max_seq_len, adapter_size=adapter_size)\n","\n","#model.load_weights(\"twitter_long_two.h5\")\n","\n","model.summary()\n","\n","total_epoch_count = 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bert shape (None, 128, 768)\n","Done loading 196 BERT weights from: ./bert/checkpoints/bert_base/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f113862b438> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n","Unused weights from checkpoint: \n","\tbert/embeddings/token_type_embeddings\n","\tbert/pooler/dense/bias\n","\tbert/pooler/dense/kernel\n","\tcls/predictions/output_bias\n","\tcls/predictions/transform/LayerNorm/beta\n","\tcls/predictions/transform/LayerNorm/gamma\n","\tcls/predictions/transform/dense/bias\n","\tcls/predictions/transform/dense/kernel\n","\tcls/seq_relationship/output_bias\n","\tcls/seq_relationship/output_weights\n","Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_ids (InputLayer)       [(None, 128)]             0         \n","_________________________________________________________________\n","bert (BertModelLayer)        (None, 128, 768)          108890112 \n","_________________________________________________________________\n","lambda (Lambda)              (None, 768)               0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 768)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 768)               590592    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 768)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 2)                 1538      \n","=================================================================\n","Total params: 109,482,242\n","Trainable params: 109,482,242\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V3CAz4O936RD","colab_type":"text"},"source":["## Currently no model to load"]},{"cell_type":"code","metadata":{"id":"yishNhR-qJPR","colab_type":"code","colab":{}},"source":["#model.load_weights(\"twitter_bert_large_second.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Idw95IXg3_yM","colab_type":"text"},"source":["## Train model/fit+save weights"]},{"cell_type":"code","metadata":{"id":"GmI1kXiOxUYi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1592381947559,"user_tz":-120,"elapsed":91873,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"24dafa9c-121d-4960-8dd5-0188b6b669a2"},"source":["model.fit(dataset_train,\n","          validation_data=dataset_test,\n","          epochs=total_epoch_count,\n","          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n","                                                  end_learn_rate=1e-5,\n","                                                  warmup_epoch_count=3,\n","                                                  total_epoch_count=total_epoch_count),\n","                    keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n","                  ])\n","\n","\n","model.save_weights('./twitter_bert_large_third.h5', overwrite=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n","35120/35120 [==============================] - 3018s 86ms/step - loss: 0.4702 - acc: 0.8358 - val_loss: 0.4415 - val_acc: 0.8663 - lr: 1.0000e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WGn6qOf_4KfK","colab_type":"text"},"source":["## ?"]},{"cell_type":"code","metadata":{"id":"Bl0M7uP60QuX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592381947564,"user_tz":-120,"elapsed":48,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"8ddb5de5-788b-41a3-ddc6-f8aba53fd57b"},"source":["'''\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n","  process = psutil.Process(os.getpid())\n","  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n","  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","\n","printm()\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# memory footprint support libraries/code\\n!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\\n!pip install gputil\\n!pip install psutil\\n!pip install humanize\\n\\nimport psutil\\nimport humanize\\nimport os\\nimport GPUtil as GPU\\nGPUs = GPU.getGPUs()\\n\\n# XXX: only one GPU on Colab and isn’t guaranteed\\ngpu = GPUs[0]\\ndef printm():\\n  process = psutil.Process(os.getpid())\\n  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\\n  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\\n\\nprintm()\\n'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"6GSmYh_299gl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592381962644,"user_tz":-120,"elapsed":15097,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"a2e6c997-54b6-49b9-cff8-61dc782ce134"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AUCypRRM4RjF","colab_type":"text"},"source":["## Preprocess test-set"]},{"cell_type":"code","metadata":{"id":"cWS4vv1VE87w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592381986041,"user_tz":-120,"elapsed":23431,"user":{"displayName":"Manuel Meinen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikyaPxXbQTJ8kGKW3qcAPhFagyA2xTSxGuD_KdzQ=s64","userId":"02636813164357111374"}},"outputId":"b6282950-50e5-4f76-bdf0-1279f8837086"},"source":["positive_data = []\n","with open(DATASET_FILE_TRAIN_POS, \"r\") as f:\n","  for x in f:\n","    positive_data.append(x)\n","\n","negative_data = []\n","with open(DATASET_FILE_TRAIN_NEG, \"r\") as f:\n","  for x in f:\n","    negative_data.append(x)\n","\n","\n","print(len(positive_data))\n","positive_data = list(dict.fromkeys(positive_data))\n","print(len(positive_data))\n","\n","print(len(negative_data))\n","negative_data = list(dict.fromkeys(negative_data))\n","print(len(negative_data))\n","test_data = []\n","with open(DATASET_FILE_TEST, \"r\") as f:\n","  for x in f:\n","    test_data.append(x)\n","\n","encoded_test_data = []\n","for x in test_data:\n","  result, _ = pre_process_positive(tf.convert_to_tensor(x), 128)\n","  encoded_test_data.append(tf.reshape(tf.convert_to_tensor(result), (1,128)))\n","encoded_test_ds = tf.data.Dataset.from_tensor_slices(encoded_test_data)\n","for x in encoded_test_ds.take(5):\n","  print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1250000\n","1127644\n","1250000\n","1142838\n","tf.Tensor(\n","[[  101  1015  1010  2712 20160  4013  2712  8040 17206  2121  1006  2998\n","   2007  1996 12109  2712  1011 20160 11915  3597 12184 22381  3726  2250\n","   1010  2994  2936  1999  1996  2300  1998  1012  1012  1012  1026 24471\n","   2140  1028   102     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n","tf.Tensor(\n","[[  101  1016  1010  1026  5310  1028 18454 10603  2092  1045  2147  2035\n","   2733  2061  2085  1045  2064  1005  1056  2272 15138  2017  2006   999\n","   2821  1998  2404  2216 10274  1999  2115 10250 19879  4263   999   999\n","    999   102     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n","tf.Tensor(\n","[[  101  1017  1010  1045  2064  2102  2994  2185  2013 11829  2008  2015\n","   2026  3336   102     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n","tf.Tensor(\n","[[  101  1018  1010  1026  5310  1028  2053  5003  1005  2572   999   999\n","    999  8840  2140 10047  6669  2986  1998  2025  9530 15900  6313  4902\n","   1048  2863  2080   102     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n","tf.Tensor(\n","[[  101  1019  1010  7188  1045  2991  6680  3666  1996  2694  1010  1045\n","   2467  5256  2039  2007  1037 14978   102     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 128), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J4k1CZtJ4pS8","colab_type":"text"},"source":["## Predict test set"]},{"cell_type":"code","metadata":{"id":"oauQeQCvJ2Qq","colab_type":"code","colab":{}},"source":["results = model.predict(encoded_test_ds)\n","labels = np.argmax(results, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FE2UiBBn4x2X","colab_type":"text"},"source":["## Postprocessing"]},{"cell_type":"code","metadata":{"id":"LdNIjYFDKBU9","colab_type":"code","colab":{}},"source":["#labels = [1 if x == 1 else -1 for x in labels]\n","\n","#reverse labels\n","labels = [-1 if x == 1 else 1 for x in labels]\n","#un-reverse labels\n","labels = [1 if x == -1 else -1 for x in labels]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vKSwMczRZCp","colab_type":"code","colab":{}},"source":["df = pd.DataFrame({'Id': np.arange(1,10001),\n","                  'Prediction': labels\n","                   })\n","df.to_csv('bert_large_1point5.csv', index=False)"],"execution_count":null,"outputs":[]}]}